# HCNet

This is the office implement of HCNet for remote sensing iamges caption, include Sydney, UCM, and the NWPU datasets.
HCNet: Hierarchical Feature Aggregation and Cross-Modality Feature Align for Remote Sensing Image Captioning

Remote sensing image captioning aims to describe the crucial objects from remote sensing images in the form of natural language. This task has garnered extensive attention from researchers due to its potential applications. Currently, it is challenging to generate high-quality captions due to the multi-scale targets in remote sensing images and the cross-modality differences between images and text features. To address these problems, this paper presents a novel approach for generating captions through hierarchical feature aggregation and cross-modality feature alignment, namely HCNet. Specifically, we propose a hierarchical feature aggregation module (HFAM) to obtain a comprehensive representation of vision features representation. Taking into account the disparities between different modality features, a cross-modality feature interaction module (CFIM) is designed in the decoder to facilitate feature alignment. At the same time, a cross-modality align loss is introduced to realize the alignment of vision and text features. Extensive experiments conduct on the Sydney, UCM, and NWPU datasets results show our HCNet can achieve satisfactory performance.
